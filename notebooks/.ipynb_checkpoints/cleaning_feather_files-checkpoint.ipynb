{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4345c960-31f3-4422-8735-008cafe03e01",
   "metadata": {},
   "source": [
    "## This section is for cleaning the tweets using the clean_tweet function.\n",
    "This section takes data from G_DATA, cleans them, and stores them in my local `./data` directory under `./politics_cleaned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da28f06-bd8d-4479-afc7-13c88b77641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import allTokens\n",
    "from allTokens import abbr_dict, emoji_pattern\n",
    "\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "\n",
    "import pandas as pd\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ac8d02-b37c-4415-89c7-93f664460a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_DATA = \"../data\"\n",
    "G_DATA = \"/DATA/BLF/politics/\"\n",
    "# G_DATA_STORE = \"/DATA/BLF/politics/CLEANED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f3c6f0-88bd-4c7a-806b-8367138bbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from src/preprocessing.py\n",
    "my_nlp = None\n",
    "\n",
    "extraChar = {'&quot;': '\"',\n",
    " '&amp;': 'and',\n",
    " '&lt;': '<',\n",
    " '&gt;': '>',\n",
    " '&nbsp;': 'un-linebreak-able space',\n",
    " '&iexcl;': '¡',\n",
    " '&cent;': '¢',\n",
    " '&pound;': '£',\n",
    " '&curren;': '¤',\n",
    " '&yen;': '¥',\n",
    " '&brvbar;': '¦',\n",
    " '&sect;': '§',\n",
    " '&uml;': '¨',\n",
    " '&copy;': '©',\n",
    " '&ordf;': 'ª',\n",
    " '&laquo;': '«',\n",
    " '&not;': '¬',\n",
    " '&shy;': '\\xad',\n",
    " '&reg;': '®',\n",
    " '&macr;': '¯',\n",
    " '&deg;': '°',\n",
    " '&plusmn;': '±',\n",
    " '&sup2': '²',\n",
    " '&sup3;': '³',\n",
    " '&acute;': '´',\n",
    " '&micro;': 'µ',\n",
    " '&para;': '¶',\n",
    " '&middot;': '·',\n",
    " '&cedil;': '¸',\n",
    " '&sup1;': '¹',\n",
    " '&ordm;': 'º',\n",
    " '&raquo;': '»',\n",
    " '&frac14;': '¼',\n",
    " '&frac12;': '½',\n",
    " '&frac34;': '¾',\n",
    " '&iquest;': '¿',\n",
    " '&times;': '×',\n",
    " '&divide;': '÷',\n",
    " '&ETH;': 'Ð',\n",
    " '&eth;': 'ð',\n",
    " '&THORN;': 'Þ',\n",
    " '&thorn;': 'þ',\n",
    " '&AElig;': 'Æ',\n",
    " '&aelig;': 'æ',\n",
    " '&OElig;': 'Œ',\n",
    " '&oelig;': 'œ',\n",
    " '&Aring;': 'Å',\n",
    " '&Oslash;': 'Ø',\n",
    " '&Ccedil;': 'Ç',\n",
    " '&ccedil;': 'ç',\n",
    " '&szlig;': 'ß',\n",
    " '&Ntilde;': 'Ñ',\n",
    " '&ntilde;': 'ñ'}\n",
    "\n",
    "special = {\n",
    "    \"’\":\"'\",\n",
    "    \"‘\":\"'\",\n",
    "    \"`\":\"'\",\n",
    "    '“':'\"',\n",
    "    '”':'\"',\n",
    "    '…':\".\"\n",
    "}\n",
    "\n",
    "emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c821aa-35b8-45e0-b1d7-218a52fc7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory('language_detector')\n",
    "def language_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "def get_nlp():\n",
    "    global my_nlp\n",
    "    if not my_nlp:        \n",
    "        my_nlp = spacy.load('en_core_web_lg')\n",
    "        my_nlp.add_pipe('language_detector')\n",
    "    return my_nlp\n",
    "\n",
    "def is_english(text):\n",
    "    doc = get_nlp()(text)\n",
    "    return doc._.language['score']>0.95\n",
    "        \n",
    "def removeTags(text,splitter):\n",
    "    div = text.split(splitter)\n",
    "    endExists = True\n",
    "    i = len(div)-1\n",
    "    while i>=0 and endExists:\n",
    "        if len(div[i].strip().split(\" \"))  == 1:\n",
    "            div.pop(i)\n",
    "            i-=1\n",
    "        else:\n",
    "            endExists = False\n",
    "        \n",
    "    return \" \"+splitter.join(div).strip()\n",
    "\n",
    "def removeTagsFromStart(text,splitter):\n",
    "    div = text.split(splitter)\n",
    "    endExists = True\n",
    "    i = 0\n",
    "    while len(div)>0 and endExists:\n",
    "        if len(div[i].strip().split(\" \"))  == 1:\n",
    "            div=div[i+1:]\n",
    "        else:\n",
    "            div[i] = splitter+div[i]\n",
    "            endExists = False\n",
    "    if len(div) == 0:\n",
    "        return ''\n",
    "    splitfirst = div[0].split(\" \")\n",
    "    if \"you\" in splitfirst[1].lower() and \"@\" in splitfirst[0]:\n",
    "        splitfirst[1] = splitfirst[0]\n",
    "        splitfirst = splitfirst[1:]\n",
    "        div[0] = \" \".join(splitfirst)\n",
    "    if \"@\" in splitfirst[0].strip()[0:2]:\n",
    "        splitfirst = splitfirst[1:]\n",
    "        div[0] = \" \".join(splitfirst)\n",
    "    return \" \".join(div).strip()\n",
    "\n",
    "\n",
    "def removeRT(text):\n",
    "    if text[0:2] == 'RT':\n",
    "        return \":\".join(text.split(\":\")[1:]).strip()\n",
    "    return text  \n",
    "\n",
    "def clean_tweet(text,removeFromMiddle):\n",
    "    text = text.strip()\n",
    "    for key,value in special.items():\n",
    "        text = re.sub(key,value,text)\n",
    "    for key,value in abbr_dict.items():\n",
    "        text = re.sub(key,value,text,flags=re.I)\n",
    "    for key,value in extraChar.items():\n",
    "        text = re.sub(key,value,text)\n",
    "    \n",
    "        #print(text)\n",
    "    if removeFromMiddle:\n",
    "        text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "        text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = emoji_pattern.sub(r' ', text)\n",
    "    text = removeTags(text,\"#\")\n",
    "    text = removeTags(text,\"@\")\n",
    "    text = removeTags(text,\"#\")\n",
    "    text = removeTagsFromStart(text,\"@\")\n",
    "    text = removeRT(text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(\"@\",'',text)\n",
    "    text = re.sub(\"#\",'',text)\n",
    "    text = re.sub(r'[\\n\\r]+',r'\\n',text)\n",
    "    text = re.sub('(?<![.?!])\\n',\". \",text)\n",
    "    text = re.sub('\\n',\" \",text)\n",
    "    #text = ' '.join(text.replace('\\r', ' ').split())\n",
    "    text = re.sub(\"\\s+\",\" \",text)\n",
    "    #text = re.sub(r\"[^A-Za-z.!?'', ]\",'',text)\n",
    "    \n",
    "    if not is_english(text):\n",
    "        return ''\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d506ff12-88a7-48c6-8bbf-370251d3a11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4351948\n"
     ]
    }
   ],
   "source": [
    "data_february = pd.read_feather(f\"{G_DATA}/output_feb.feather\")\n",
    "data_march = pd.read_feather(f\"{G_DATA}/output_mar.feather\")\n",
    "data_april = pd.read_feather(f\"{G_DATA}/output_april.feather\")\n",
    "data_may = pd.read_feather(f\"{G_DATA}/output_may.feather\")\n",
    "data_june = pd.read_feather(f\"{G_DATA}/output_jun.feather\")\n",
    "data_july = pd.read_feather(f\"{G_DATA}/output_jul.feather\")\n",
    "data_august = pd.read_feather(f\"{G_DATA}/output_aug.feather\")\n",
    "data_september = pd.read_feather(f\"{G_DATA}/output_sept.feather\")\n",
    "data_october = pd.read_feather(f\"{G_DATA}/output_oct.feather\")\n",
    "data_november = pd.read_feather(f\"{G_DATA}/output_nov.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3134059c-4b56-4f0f-b7dc-7348a762b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetCleaner(df):\n",
    "    ogtext = df['text'].tolist()\n",
    "    ans = []\n",
    "    for i in range(len(ogtext)):\n",
    "        ans.append(clean_tweet(ogtext[i],True))\n",
    "    df['cleanedText'] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a26c0-b65d-47c1-b16b-8c8f85ab1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread1 = Thread(target=lambda: tweetCleaner(data_february).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_february_rohan.feather\"))\n",
    "thread2 = Thread(target=lambda: tweetCleaner(data_march).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_march_rohan.feather\"))\n",
    "thread3 = Thread(target=lambda: tweetCleaner(data_april).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_april_rohan.feather\"))\n",
    "thread4 = Thread(target=lambda: tweetCleaner(data_may).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_may_rohan.feather\"))\n",
    "thread5 = Thread(target=lambda: tweetCleaner(data_june).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_june_rohan.feather\"))\n",
    "thread6 = Thread(target=lambda: tweetCleaner(data_july).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_july_rohan.feather\"))\n",
    "thread7 = Thread(target=lambda: tweetCleaner(data_august).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_august_rohan.feather\"))\n",
    "thread8 = Thread(target=lambda: tweetCleaner(data_september).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_september_rohan.feather\"))\n",
    "thread9 = Thread(target=lambda: tweetCleaner(data_october).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_october_rohan.feather\"))\n",
    "thread10 = Thread(target=lambda: tweetCleaner(data_november).reset_index(drop=True).to_feather(f\"{L_DATA}/output_cleaned_november_rohan.feather\"))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "thread4.start()\n",
    "thread5.start()\n",
    "thread6.start()\n",
    "thread7.start()\n",
    "thread8.start()\n",
    "thread9.start()\n",
    "thread10.start()\n",
    "\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "thread4.join()\n",
    "thread5.join()\n",
    "thread6.join()\n",
    "thread7.join()\n",
    "thread8.join()\n",
    "thread9.join()\n",
    "thread10.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
